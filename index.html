<!DOCTYPE html>
<html>
	<head>
		<title>Benjamin XY.Q</title>
	</head>
	<body>
<div class="limiter">

<div class="mainbox" style="padding-left:10px">
<table style="font-size:100%;padding:0px;margin:0px;"><tbody><tr valign="top"><td>
<img src="head/2_21.jpg" style="border: 1px solid rgb(0, 0, 0); height: 175px; --darkreader-inline-border-top:#8c8273; --darkreader-inline-border-right:#8c8273; --darkreader-inline-border-bottom:#8c8273; --darkreader-inline-border-left:#8c8273;" id="head" data-darkreader-inline-border-top="" data-darkreader-inline-border-right="" data-darkreader-inline-border-bottom="" data-darkreader-inline-border-left="">
</td><td>
<b>David Fouhey</b><br>
<b>Quick info:</b>
<a href="docs/cv.pdf">CV</a> &nbsp; 
<a href="http://scholar.google.com/citations?user=FLcpd34AAAAJ">Google Scholar</a> &nbsp;
<br>
<b>Email (<a href="#" onclick="descramble()">Reveal</a>):</b> <span id="emailHolder">xdgtnezstcd.gbhlt</span> (my Berkeley &amp; CMU emails do not work)<br>
<a href="#" onclick="document.getElementById(&quot;joining&quot;).style.display=&quot;block&quot;;">About joining my research group <b>(please read before emailing)</b></a> 
<br>
<b>Physical Location:</b> Beyster (BBB) 3777, University of Michigan<br>
<noscript>If you're the type of person who browses without javascript, your first guess for my email is probably going to be right.<br/></noscript>
<b>Name FAQ:</b> `foe'-`eee'. It rhymes with snowy or Joey: the key is to forget how it is spelled. It (but not me) is from County Cork, Ireland. 
<br>
<b>Photos:</b> One picture is hard to identify a person with. <a href="pics/index.htm">Here</a> are some more.
</td></tr></tbody></table> 
<p><i>Summary:</i> 
I am an Assistant Professor of 
Computer Science and Engineering at
<a href="https://www.cse.umich.edu/">
The University of Michigan</a>. 
I was previously a postdoctoral fellow at
<a href="https://eecs.berkeley.edu/">UC Berkeley</a>, 
working with
<a href="http://people.eecs.berkeley.edu/~efros/">Alyosha Efros</a>
and
<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>.
Before that, I received a Ph.D. in robotics from
<a href="http://www.ri.cmu.edu/">CMU</a> at
where I worked with <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>
and <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>.
<!--                   
I <b>was</b> a postdoctoral fellow (2016-)
at UC Berkeley, working with 
I received a Ph.D. in Robotics (2011-2016) from
<a href='http://www.ri.cmu.edu/'>The Robotics Institute</a> at
<a href='http://cmu.edu/'>Carnegie Mellon University</a>, where
I worked with <a href='http://www.cs.cmu.edu/~abhinavg/'>Abhinav Gupta</a>
and <a href='http://www.cs.cmu.edu/~hebert/'>Martial Hebert</a> and
was a <a href='https://www.nsfgrfp.org/'>NSF</a>
and then <a href='http://ndseg.asee.org'>NDSEG</a> fellow. 
Now I am elsewhere!
-->
</p>
<p>
For more information about computer vision at Michigan, see
<a href="http://vision.eecs.umich.edu/">here</a>.
</p>
</div>

<div id="joining" class="mainbox contentbox" style="display:none">
<h3>About Joining My Group</h3>
<br>
Long story short, I am always looking for motivated and talented students.
I also receive hundreds of emails a day and cannot respond to
all inquiries about joining my group. This document is not meant to be
unfriendly, but just to save both of us time (many of these things are things
I'd say in an email back).  
<ul>
<li><b>If you are a UM student</b>:<br> 
Until Fall 2021, (a) I will likely not have capacity for new mentees except through
a small number of undergraduate research programs and (b) I will almost certainly not
be taking on new MS students. Feel free
to email other faculty in vision and learning, as well as graduate students.
<!-- 
Contact me by email and let's talk!  
But to save time, please include your resume, an unofficial transcript, and
why you're interested. I don't always
have the bandwidth to take on new mentees, but if I don't, I'll try to point
you in the right direction of someone who might have the bandwidth. I usually do a good job 
of responding to these emails and if I haven't, let me know.
-->
<br>
<br>
</li><li><b>If you are not in a UM degree program and would like to be my student</b>:<br>
Please <a href="https://www.eecs.umich.edu/eecs/graduate/cse/apply/">apply directly</a> 
and mention my name (as well as others that interest you) in your research statement. That's
all that is required! Here are answers to common questions:
<ul>
<li>I am always looking for students and will almost always be recruiting and open to taking on the right student
</li><li>I am recruiting extraordinarily selectively for the forseeable future, although keep in mind there are many people who do vision at Michigan
</li><li>Admissions in anything related to computer vision and machine learning is
currently extraordinarily competitive and I cannot assess your chances for you -- 
it depends highly on my group's needs at the moment and the full pool of applicants.
However, you can look at current vision PhD students at Michigan to get a sense of typical profiles.
</li><li>Emailing me genuinely does not improve your chances.
</li><li>I genuinely read all PhD applications that mention my name, but 
applications are read and decided on by a central committee (I encourage reading <a href="http://www.cs.cmu.edu/~harchol/gradschooltalk.pdf">Mor Harchol-Balter's guide to graduate school admissions in CS</a> for a good sense of how this process works)
</li><li>I am not involved in the MS process at all
</li><li>I am delighted to talk to people who have been admitted; I cannot talk to people before they are admitted due to time constraints (in 2020, I was listed on &gt; 200 of the applications to the UM CSE PhD program).
</li><li>I cannot reveal information about the process or give status updates.
</li><li>I do not respond to most emails about this.
</li></ul>
<br>
</li><li><b>If you are not in a UM degree program and would like to visit</b>:<br>
I do host visitors occasionally but have obligations to my students first and foremost, as
well as to UM students. Here are answers to common questions:
<ul>
<li>I am not looking for visitors without a recommendation from somebody <i>I personally know or know of</i>. If you are currently
working with someone, please tell me who.
</li><li>I do not advise capstone projects
</li><li>I do not take any short-term (e.g., less-than-three month) undergraduate visitors except in highly exceptional circumstances.
</li><li>I do not respond to the overwhelming majority of emails about this.
</li></ul>
</li></ul>
If you've read this page and think you have a question that has not been answered, please feel free to email
and put ``(JJ Gibson)'' in the subject line. I will always do my best to answer these.
<br><br>

</div>


<div class="navcontainer">
<div class="buttonbox"><a href="#news">News</a></div>
<div class="buttonbox"><a href="#students">Students</a></div>
<div class="buttonbox"><a href="#research">Research</a></div>
<div class="buttonbox"><a href="#teaching">Teaching</a></div>
<div class="buttonbox"><a href="#publications">Publications</a></div>
<div class="buttonbox"><a href="#miscellaneous">Misc.</a></div>
<div class="buttonbox"><a href="#fun">Fun</a></div>
</div>




<a name="news"></a>
<div class="mainbox contentbox">
<h3>News</h3>
<ul>
<li>Paper accepted in APJ on emulating the SDO/HMI pipeline. Congratulations to Richard ☀️!
</li><li>Two papers accepted at ECCV. Congratulations to Shengyi, Linyi, and Chris!<br>
</li><li>Paper accepted at MLHC. Congratulations Sarah!<br>
</li><li>Three papers accepted at CVPR. Congratulations to Dandan, Mohamed, Nilesh, Jiaqi and Michelle!<br>
We are also releasing pre-release versions of our model for hand state detection -- contact me if you are interested. 
</li><li>Our paper on predicting extreme UV solar irradiance is out and 
<a href="https://www.techexplorist.com/monitoring-suns-ultraviolet-emission-using-deep-learning/26846/">getting</a>
<a href="https://www.inverse.com/article/59765-a-key-nasa-discovery-could-protect-mega-constellations-like-spacex-starlink">some</a>
<a href="https://www.theregister.co.uk/2019/10/03/nasa_sun_ai/">press</a>
<a href="https://phys.org/news/2019-10-team-deep-sun-ultraviolet-emission.html">coverage</a>. There are some more down-to-earth things in the pipeline too...
<!--
<li>Stop using contrived versions of MNIST to test your algorithms!
Check out our new dataset of 8 years of
solar data <a href='https://github.com/dfouhey/sdodemo'>here</a>,
including 60K observations in 512x512 glory across 12 modalities (plus
14 scalar labels too). 
<li>I started at the University of Michigan.
<li>I visited 
Josef Sivic and Ivan Laptev in 
INRIA, <a href='http://www.di.ens.fr/willow/'>Willow Group</a>,
and had a wonderful time.  
<li>Check out our new video dataset, <a href='2017/VLOG/'>VLOG</a>
<li>The next iteration of our <a href='http://bridgesto3d.github.io/'>Bridges to 3D</a> workshop, which is about how 3D vision can help and be helped by other fields, will appear at CVPR 2018. 
<li>I'm on the job market, looking for a tenure-track job!
<li>I organized a workshop, <a href='https://bridgesto3d.github.io/'>Bridges to 3D</a>, at CVPR 2017 about how 3D vision can help and be helped by other fields with <a href='http://www.cs.utexas.edu/~huangqx/'>Qixing Huang</a>
and <a href='http://people.csail.mit.edu/lim/'>Joseph Lim</a>. 
<li><a href='http://www.oneweirdkerneltrick.com/rank.pdf'>Visual Rank Estimation</a>
(<i>N.B.: not a real paper</i>) featured on K&aacute;roly Zsolnai-Feh&eacute;r's 
<a href='https://www.youtube.com/watch?v=bLFISzfQCDQ'>Two Minute Paper</a>
Video Series!
<li>I started a postdoc in Fall 2016 at UC Berkeley with Jitendra Malik and Alyosha Efros.
-->
<!--
<li>My picks for CVPR 2016 were featured in <a href='http://www.rsipvision.com/CVPR2016-Tuesday/'>CVPR Daily News</a>
<li>I co-taught <a href='http://graphics.cs.cmu.edu/courses/16-824/2016_spring/'>16-824 (Visual Learning and Recognition)</a> in Spring 2016.
<li> I visited the <a href='http://www.robots.ox.ac.uk/~vgg/'>VGG group at Oxford</a> for Summer 2015.
-->
</li></ul>
</div>

<a name="research"></a>
<div class="mainbox contentbox">
<h3>Research</h3>
I work on computer vision and machine learning.
I want to develop autonomous systems that can learn to build representations
of the underlying state and dynamics of the world through observation (and
potentially interaction). <br><br>

Towards this end, 
I'm particularly interested in two aspects of vision.
Both are motivated by the somewhat obvious observation that images are the projection of a real, physical, and 3D
world to a 2D image plane of an agent. This has lead to research on
understanding the world in terms of:
<ul>
<li><b>physical properties</b>: How do we recover a rich 3D world from a 2D image? 
I am especially interested in representations -- the answers that are obvious
are also obviously defective -- as well as how we should reconcile our strong
prior knowledge about this structure of the problem with data-driven
techniques.<br><br>
I've lately become interested in applying this more broadly with the
hope that we can develop AI systems that can learn how the physical world
works from observation, including work on solar physics.<br><br>

</li><li><b>functional properties</b>: How do we infer and
understand opportunities for
interaction? I am interested how an agent (e.g., human or robot) can
interact with the world, including in terms of what this implies for 3D understanding.
</li></ul>
<br>
My group's work is sponsored by 
<a href="https://www.darpa.mil/">DARPA</a>, 
<a href="https://www.tri.global/">Toyota Research Institute</a>, 
<a href="http://pg.com/">The Procter &amp; Gamble Company</a>,
<a href="https://nsf.gov/">The National Science Foundation</a>,
<a href="https://www.nokia.com/networks/">Nokia Networks</a>,
<a href="http://nasa.gov/">NASA</a>,
and the <a href="https://www.lmsal.com/">Lockheed Martin Solar and Astrophysics Laboratory</a>
as well as the University of Michigan's Precision Health Initiative and 
the Michigan Institute for Data Science.
<br>
<br>
</div>

<a name="students"></a>
<div class="mainbox contentbox">
<h3>Student Collaborators</h3>
A full (and maybe up to date) list of people I work with is <a href="http://fouheylab.eecs.umich.edu/people.html">here</a>, but my current
PhD student collaborators are:
<ul>
<li><a href="http://relh.net/">Richard Higgins</a>, Sep 2019 — present
</li><li><a href="https://nileshkulkarni.github.io/">Nilesh Kulkarni</a>, (co-advised with <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>), Sep 2019 — present
</li><li><a href="https://jasonqsy.github.io/">Shengyi Qian</a>, Sep 2019 — present
</li><li><a href="https://ddshan.github.io/">Dandan Shan</a>, Sep 2020 — present, previously ECE MS Student with me
</li><li><a href="https://crockwell.github.io/">Christopher Rockwell</a>, (co-advised with <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>), Sep 2020 — present, previously CSE MS Student with me
</li><li><a href="https://sjabbour.github.io/">Sarah Jabbour</a>, (co-advised with <a href="http://www-personal.umich.edu/~wiensj/">Jenna Wiens</a>), Sep 2020 — present
</li></ul>
My current MS Student collaborators are:
<ul>
<li><a href="https://jinlinyi.github.io/">Linyi Jin</a>, May 2019 — present,
</li><li>Max Hamilton, Sep 2020 — present, previously CSE BS Student with me
</li><li>Yinwei Dai, Sep 2020 — present
</li></ul>
</div>


<a name="teaching"></a>
<div class="mainbox contentbox">
<h3>Teaching</h3>


University of Michigan:
<ul>
<li>EECS 542 (Advanced Topics in Computer Vision): <a href="teaching/EECS542_F20/">Fall 2020</a>
</li><li>EECS 598 (Special Topics: The Ecological Approach to Visual Perception):
<a href="teaching/EECS598_W20/">Winter 2020</a>
</li><li>EECS 442 (Computer Vision): 
    <a href="teaching/EECS442_F19/">Fall 2019</a>, 
    <a href="teaching/EECS442_W19/">Winter 2019</a>
</li><li><a href="https://cedo.engin.umich.edu/ai4all/">AI4All</a> (a two-week residential program for high schoolers): Summer 2019 -- present
</li></ul>
Earlier:
<ul>
<li>UC Berkeley 294-43 (Object and Activity Recognition) <a href="http://sites.google.com/site/ucbcs29443/">Spring 2018</a>,
<a href="https://sites.google.com/site/ucbcs29443fall2017/">Fall 2017</a>,
<a href="https://sites.google.com/site/ucbcs29443spring2017/">Spring 2017</a>. With Trevor Darrell and Alexei Efros.
</li><li>CMU <a href="http://graphics.cs.cmu.edu/courses/16-824/2016_spring/">16-824 (Visual Learning and Recognition)</a>, Spring 2016. 
With Abhinav Gupta.
</li><li><a href="http://www.cs.cmu.edu/~dfouhey/ECCV2014Tutorial/">Tutorial on 3D Scene Understanding</a> 
at ECCV 2014 with 
<a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>, <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>, and 
<a href="http://dhoiem.cs.illinois.edu/">Derek Hoiem</a>.
<a <br=""><br></a></li></ul></div><a <br="">


</a><a name="publications"></a>
<div class="mainbox contentbox">
<h3>Selected Publications</h3>

<table cellspacing="20">

<tbody><tr><td>
<span style="font-size:150%;font-weight:bold">2021</span>
</td></tr>

<tr><td>
<img src="2021/emulation/teaser.png" width="300">
</td><td>
R.E.L. Higgins, <b>D.F. Fouhey</b>, D. Zhang, Spiro K. Antiochos, Graham Barnes, Todd Hoeskema, K.D. Leka, Yang Liu, Peter W. Schuck, Tamas I. Gombosi<br>
<i>Fast and Accurate Emulation of the SDO/HMI Stokes Inversion with Uncertainty Quantification</i><br>
Accepted in The Astrophysical Journal, 2021<br>
[Coming Soon!]
</td></tr>

<tr><td>
<span style="font-size:150%;font-weight:bold">2020</span>
</td></tr>

<tr><td>
<img src="2020/assoc3d/teaser.png" width="300">
</td><td>
S. Qian*, L. Jin*, <b>D. F. Fouhey</b><br>
<i>Associative3D: Volumetric Reconstruction from Sparse Views</i><br>
ECCV 2020<br>
[<a href="https://arxiv.org/abs/2007.13727">Arxiv</a>] &nbsp; 
[<a href="https://jasonqsy.github.io/Associative3D/">Project Page</a>] &nbsp;
[<a href="https://github.com/JasonQSY/Associative3D">Code</a>] &nbsp; 
[<a href="2020/assoc3d/qian2020.bib">Bibtex</a>]
</td></tr>


<tr><td>
<img src="2020/partialhumans/teaser.png" width="300">
</td><td>
C. Rockwell, <b>D. F. Fouhey</b><br>
<i>Full-Body Awareness from Partial Observations</i><br>
ECCV 2020<br>
[<a href="https://arxiv.org/abs/2008.06046">Arxiv</a>] &nbsp;
[<a href="https://crockwell.github.io/partial_humans/">Project Page</a>] &nbsp;
[<a href="2020/partialhumans/rockwell2020.bib">Bibtex</a>] &nbsp;
[<a href="https://github.com/crockwell/partial_humans/">Code</a>]
</td></tr><tr><td>
<img src="2020/mlhc/teaser.png" width="300">
</td><td>
S. Jabbour, <b>D.F. Fouhey</b>, E. Kazerooni, M.W. Sjoding, J. Wiens<br> 
<i>Deep Learning Applied to Chest X-Rays: Exploiting and Preventing Shortcuts</i><br>
MLHC 2020
<br>
[<a href="2020/mlhc/jabbour20.pdf">PDF</a>] &nbsp;
[<a href="2020/mlhc/jabbour20.bib">Bibtex</a>] &nbsp;
[<a href="https://gitlab.eecs.umich.edu/mld3/deep-learning-applied-to-chest-x-rays-exploiting-and-preventing-shortcuts">Code</a>]
</td></tr>

<tr><td>
<img src="2020/100DOH/teaser.jpg" width="300">
</td><td>
D. Shan, J. Geng*, M. Shu*, <b>D.F. Fouhey</b><br>
<i>Understanding Human Hands in Contact at Internet Scale</i><br>
CVPR 2020 (<span style="color: rgb(170, 0, 0); --darkreader-inline-color:#ff5555;" data-darkreader-inline-color=""><b>Oral</b></span>) 
<br>
[<a href="2020/100DOH/hands.pdf">PDF</a>] &nbsp;
[<a href="2020/100DOH/shan20.bib">Bibtex</a>] &nbsp;
[<a href="https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/">Project Page &amp; Code</a>]
</td></tr>

<tr><td>
<img src="2020/novelVP/teaser.jpg" width="300">
</td><td>
M. El Banani, J. Corso, <b>D.F. Fouhey</b><br>
<i>Novel Object Viewpoint Estimation through Reconstruction Alignment</i><br>
CVPR 2020<br>
[<a href="2020/novelVP/novelvp.pdf">PDF</a>] &nbsp; 
[<a href="2020/novelVP/novelvp-supp.pdf">Supp.</a>] &nbsp; 
[<a href="2020/novelVP/elbanani20.bib">Bibtex</a>] &nbsp; 
[<a href="https://github.com/mbanani/novelviewpoints">Code and Project Page</a>]
</td></tr>

<tr><td>
<img src="2020/acsm/teaser.jpg" width="300">
</td><td>
N. Kulkarni, A. Gupta, <b>D.F. Fouhey</b>, S. Tulsiani<br>
<i>Articulation-aware Canonical Surface Mapping</i><br>
CVPR 2020<br>
[<a href="https://arxiv.org/abs/2004.00614">Arxiv</a>] &nbsp;
[<a href="https://arxiv.org/pdf/2004.00614.pdf">PDF</a>] &nbsp; 
[<a href="2020/acsm/kulkarni20.bib">Bibtex</a>]
</td></tr>

<tr><td>
<img src="2020/CSCW/teaser.png" width="300">
</td><td>
J.Y. Song, J.J.Y. Chung, <b>D.F. Fouhey</b>, W.S. Lasecki<br>
<i>C-Reference: Improving 2D to 3D Object Pose EstimationAccuracy via Crowdsourced Joint Object Estimation</i><br>
CSCW (Computer-supported Cooperative Work) 2020<br>
[<a href="2020/CSCW/song2020.pdf">PDF</a>] &nbsp;
[<a href="2020/CSCW/song2020.bib">Bibtex</a>]
</td></tr>

<tr><td>
<span style="font-size:150%;font-weight:bold">2019</span>
</td></tr>


<tr><td>
<img src="2019/sa/teaser.jpg" width="300"> 
</td><td>
A. Szenicer*, <b>D.F. Fouhey*</b>, A. Munoz-Jaramillo, P.J. Wright, R. Thomas, R. Galvez, M. Jin, M.C.M. Cheung<br>
<i>A Deep Learning Virtual Instrument for Monitoring Extreme UV Solar Spectral Irradiance</i><br>
Science Advances, Vol. 5, Number 10, 2019<br>
[<a href="https://advances.sciencemag.org/content/5/10/eaaw6548">Open Acess</a><a>] &nbsp;
[</a><a href="2019/sa/szenicer19.bib">Bibtex</a>] &nbsp;
[<a href="2019/sa/media/0.htm">Prediction Video</a>] &nbsp; 
[<a href="2019/sa/media/1.htm">Activations Video</a>] &nbsp; 
[<a href="2019/sa/media/2.htm">Overview Video</a>] &nbsp; 
</td></tr>
<tr><td> </td><td>
Press coverage/releases:<br>
<a href="https://www.inverse.com/article/59765-a-key-nasa-discovery-could-protect-mega-constellations-like-spacex-starlink">
 <img src="2019/sa/coverage/inverse.png" alt="inverse.com" height="30">(Inverse.com)</a> &nbsp;
<a href="https://seti.org/press-release/nasa-frontier-development-lab-uses-deep-learning-monitor-suns-ultraviolet-emission">
<img src="2019/sa/coverage/seti.png" alt="SETI institute" height="30">(SETI Institute)</a><br>
<a href="https://www.theregister.co.uk/2019/10/03/nasa_sun_ai/">
<img src="2019/sa/coverage/register.png" alt="theregister.co.uk" height="30">(theregister.co.uk)</a> &nbsp;
<a href="https://phys.org/news/2019-10-team-deep-sun-ultraviolet-emission.html">
<img src="2019/sa/coverage/phys.org.png" alt="phys.org" height="30">(phys.org)</a><br>
<a href="https://www.techexplorist.com/monitoring-suns-ultraviolet-emission-using-deep-learning/26846/">
<img src="2019/sa/coverage/techexplorist.png" alt="techexplorist" height="30">(TechExplorist.com)</a> &nbsp;
<a href="https://www.scientificamerican.com/article/black-holes-volcanic-scrolls-and-a-teeny-tiny-heartbeat-science-gifs-to-start-your-week/">
<img src="2019/sa/coverage/sa.png" alt="scientific american" height="30">(ScientificAmerican.com)</a><br>
<a href="https://eos.org/articles/virtual-super-instrument-enhances-solar-spacecraft">
<img src="2019/sa/coverage/eos.png" alt="eos.org" height="30">(Earth &amp; Space Science News)</a> &nbsp;
<a href="https://www.hpcwire.com/2019/10/17/nasa-uses-deep-learning-to-monitor-solar-weather/">
<img src="2019/sa/coverage/hpcwire_logo.png" height="30">(hpcwire.com)</a><br>
<a href="https://www.sciencedaily.com/releases/2019/10/191007103609.htm">
<img src="2019/sa/coverage/sd.png" height="30">(sciencedaiy.com)</a>
</td></tr> 

<!-- Instructional -->
<tr><td>
<img src="2019/instructional/teaser.jpg" width="300">
</td><td>
D. Zhukov, J.-B. Alayrac, G. Cinbis, <b>D.F. Fouhey</b>, I. Laptev, J. Sivic<br>
<i>Cross-task weakly-supervised learning from instructional videos</i><br>
CVPR 2019<br>
[<a href="2019/instructional/paper.pdf">PDF</a>] &nbsp;
[<a href="https://github.com/DmZhukov/CrossTask">Project Page</a>] &nbsp;
[<a href="https://arxiv.org/abs/1903.08225">Arxiv</a>] &nbsp;
[<a href="2019/instructional/zhukov19.bib">Bibtex</a>] 


</td></tr><tr><td>
<!-- APJs -->
</td></tr><tr><td>
<img src="2019/apjs/teaser.jpg" width="300">
</td><td>
R. Galvez*, <b>D.F. Fouhey*</b>, M. Jin, A. Szenicer, A. Munoz-Jaramillo, M.C.M. Cheung, P.J. Wright, M.G.
Bobra, Y. Liu, J. Mason, R. Thomas<br>
<i>A Machine Learning Dataset Prepared From the NASA Solar Dynamics Observatory Mission</i><br>
The Astrophysical Journal Supplement, 242:1, 2019<br>
[<a href="https://iopscience.iop.org/article/10.3847/1538-4365/ab1005/pdf">PDF</a>] &nbsp; 
[<a href="https://arxiv.org/abs/1903.04538">Arxiv</a>] &nbsp; 
[<a href="2019/apjs/galvez19.bib">Bibtex</a>] &nbsp; 
[<a href="2019/apjs/vis.mp4">Movie</a> &amp; <a href="2019/apjs/vis.txt">Explanation</a>] &nbsp; 
[<a href="https://github.com/dfouhey/sdodemo">Small dataset + demo</a>]<br>
</td></tr> 

<tr><td>
<span style="font-size:150%;font-weight:bold">Earlier</span>
</td></tr>


<!-- Path Following -->
<tr><td>
<img src="2018/navigation/teaser.jpg" width="300">
</td><td>
A. Kumar, S. Gupta, <b>D. F. Fouhey</b>, S. Levine, J. Malik<br>
<i>Visual Memory for Robust Path Following</i><br>
NeurIPS 2018 (<span style="color: rgb(170, 0, 0); --darkreader-inline-color:#ff5555;" data-darkreader-inline-color=""><b>Oral</b></span>)<br>
[<a href="https://ashishkumar1993.github.io/rpf/">Project Page</a>] &nbsp;
[<a href="https://saurabhg.web.illinois.edu/pdfs/kumar2018visual.pdf">PDF</a>] &nbsp; 
[<a href="2018/navigation/kumar18.bib">Bibtex</a>] &nbsp;
</td></tr>


<!-- VLOG -->
<tr><td>
<img src="2017/VLOG/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, W. Kuo, A. A. Efros, J. Malik<br>
<i>From Lifestyle VLOGs to Everyday Interactions</i><br>
CVPR 2018<br>
[<a href="2017/VLOG/index.html">Project Page</a>] &nbsp;
[<a href="https://arxiv.org/abs/1712.02310">Arxiv</a>]  &nbsp;
[<a href="2017/VLOG/fouhey17b.bib">Bibtex</a>] &nbsp; 
</td></tr>


<!-- Factored 3D -->
<tr><td>
<img src="2017/factored3d/teaser.jpg" width="300">
</td><td>
S. Tulsiani, S. Gupta, <b>D. F. Fouhey</b>, A. A. Efros, J. Malik<br>
<i>Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene</i><br>
CVPR 2018<br>
[<a href="https://shubhtuls.github.io/factored3d/">Project Page</a>] &nbsp;
[<a href="https://arxiv.org/abs/1712.01812">Arxiv</a>] &nbsp;
[<a href="2017/factored3D/tulsiani17.bib">Bibtex</a>] <br>
</td></tr>

<tr><td>
<img src="2018/geons/teaser.jpg" width="300">
</td><td>
M. Lescroart, <b>D. F. Fouhey</b>, J. Malik<br>
<i>Convolutional neural networks represent shape dimensions -- but not as accurately as humans</i>
<br>
Abstract at VSS 2018<br> 
[<a href="https://jov.arvojournals.org/article.aspx?articleid=2699413&amp;resultClick=1">Abstract</a>]
</td></tr>


<!-- Visual Navigation -->
<tr><td align="center">
<img src="2017/VisualNav/teaser.jpg" width="250">
</td><td>
S. Gupta, <b>D.F. Fouhey</b>, S. Levine, J. Malik<br>
<i> Unifying Map and Landmark Based Representations for Visual Navigation</i><br>
Arxiv 2017<br>
[<a href="https://s-gupta.github.io/cmpl/">Project Page</a>] &nbsp;
[<a href="https://arxiv.org/abs/1712.08125">Arxiv</a>] &nbsp;
[<a href="2017/VisualNav/gupta17.bib">Bibtex</a>]
</td></tr>




<!-- Shape Attributes Journal Arxiv -->
<tr><td>
<img src="2017/shapeAttrJ/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, A. Gupta, A. Zisserman<br>
<i>From Images to 3D Shape Attributes</i><br>
TPAMI (Pre-print on Arxiv)<br>
The TPAMI version has ugly typesetting (full-width tables on the bottom?)
that I was unable to change. <b>Read the Arxiv one.</b><br>
[<a href="https://arxiv.org/abs/1612.06836">Arxiv</a>] &nbsp;
[<a href="2017/shapeAttrJ/fouhey16b.bib">Bibtex</a>]
</td></tr>

<!-- TL Networks -->
<tr><td>
<img src="2016/tlnetworks/teaser.jpg" width="300">
</td><td>
R. Girdhar, <b>D. F. Fouhey</b>, M. Rodriguez, A. Gupta<br>
<i>Learning a Predictable and Generative Vector Representation for Objects</i><br>
ECCV 2016  (<span style="color: rgb(0, 0, 170); --darkreader-inline-color:#669eff;" data-darkreader-inline-color=""><b>Spotlight</b></span>)<br>
[<a href="2016/tlnetworks/tlnetworks.pdf">Publication (PDF)</a>] &nbsp;
[<a href="2016/tlnetworks/girdhar16b.bib">Bibtex</a>] <br>
[<a href="https://rohitgirdhar.github.io/GenerativePredictableVoxels/">Project Page</a>]

</td></tr>

<tr><td>
<img src="2016/dissertation/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b><br>
<i>Factoring Scenes into 3D Structure and Style</i><br>
Doctoral Dissertation<br>
[<a href="2016/dissertation/dissertation.pdf">Dissertation (PDF)</a>] &nbsp;
[<a href="2016/dissertation/fouhey16b.bib">Bibtex</a>] <br>
[<a href="2016/dissertation/dissertation_talk.pdf">Defense Slides (PDF)</a>]
</td></tr>

<!-- 3D Shape Attributes -->
<tr><td>
<img src="2016/shapeAttr/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, A. Gupta, A. Zisserman<br>
<i>3D Shape Attributes</i><br>
CVPR 2016 (<span style="color: rgb(170, 0, 0); --darkreader-inline-color:#ff5555;" data-darkreader-inline-color=""><b>Oral</b></span> - 
<a href="https://www.youtube.com/watch?v=0jW8lIYNbjo">Watch the presentation on Youtube</a>)
<br>
[<a href="2016/shapeAttr/shapeAttr.pdf">Publication (PDF)</a>] &nbsp;
[<a href="2016/shapeAttr/fouhey16.bib">Bibtex</a>] <br>
[<a href="http://www.robots.ox.ac.uk/~vgg/data/sculptures/">Project Page</a>] &nbsp;
[<a href="2016/shapeAttr/sa_poster.pdf">Poster (PDF)</a>] &nbsp; 
[<a href="2016/shapeAttr/talk_final.pptx">Talk (PPTX)</a>] &nbsp; 
[<a href="2016/shapeAttr/talk_final.pdf">Talk (PDF)</a>] &nbsp; 
</td></tr>

<!-- Memex -->
<tr><td>
<img src="2016/clutter/teaser.jpg" width="300">
</td><td>
R. Girdhar, <b>D. F. Fouhey</b>, K. M. Kitani, A. Gupta, M. Hebert<br> 
<i>Cutting through the Clutter: Task-Relevant Features for Image Matching</i><br>
WACV 2016<br>
[<a href="2016/clutter/clutter.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2016/clutter/girdhar16.bib">Bibtex</a>]
</td></tr>

<!-- Unsupervised 3D -->
<tr><td>
<img src="2015/no3d/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, W. Hussain, A. Gupta, M. Hebert<br>
<i>Single Image 3D Without a Single 3D Image</i><br>
ICCV 2015<br>
[<a href="2015/no3d/no3d.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2015/no3d/fouhey15.bib">Bibtex</a>] <br>
[<a href="2015/no3d/no3d_poster.pdf">Poster (PDF)</a>] &nbsp;
[<a href="2015/no3d/supp.pdf">Supplemental (PDF)</a>] &nbsp;
[<a href="2015/no3d/suppDetails.pdf">Bonus Details (PDF)</a>] 
</td></tr>


<!-- Designing 3D -->
<tr><td>
<img src="2015/deep3d/teaser.jpg" width="300">
</td><td>
X. Wang, <b>D. F. Fouhey</b>, A. Gupta<br>
<i>Designing Deep Networks for Surface Normal Estimation</i><br>
CVPR 2015<br>
[<a href="2015/deep3d/deep3d.pdf">Publication (PDF)</a>] &nbsp;
[<a href="2015/deep3d/wang15.bib">Bibtex</a>]
</td></tr>


<tr><td align="center">
<img src="2014/origami/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, A. Gupta, and M. Hebert<br>
<i>Unfolding an Indoor Origami World</i><br>
ECCV 2014
(<span style="color: rgb(170, 0, 0); --darkreader-inline-color:#ff5555;" data-darkreader-inline-color=""><b>Oral - <a href="http://videolectures.net/eccv2014_ford_fouhey_origami_world/?q=fouhey">Watch the presentation on VideoLectures.net</a></b></span>)<br>
[<a href="2014/origami/dfouhey_origami.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2014/origami/fouhey14c.bib">Bibtex</a>] <br>
[<a href="2014/origami/index.html">Project Page</a>] &nbsp; 
[<a href="2014/origami/scgGal.pdf">Extended Results (PDF)</a>] &nbsp; 
</td></tr>

<!--Scene Dynamics-->
<tr><td>
<img src="2014/dynamics/teaser.jpg">
</td><td>
<b>D. F. Fouhey</b> and C.L. Zitnick<br>
<i>Predicting Object Dynamics in Scenes</i><br>
CVPR 2014<br>
[<a href="2014/dynamics/fouhey_zitnick_dynamics.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2014/dynamics/fouhey14b.bib">Bibtex</a>] <br>
<!-- [<a href='http://research.microsoft.com/en-us/um/people/larryz/clipart/abstract_scenes.html'>Project Page]</a> &nbsp;  -->
[<a href="2014/dynamics/fouhey_zitnick_poster.pdf">Poster (PDF)</a>] &nbsp; 
[<a href="2014/dynamics/fouhey_zitnick_supp.pdf">Supplemental (PDF)</a>]  
<br>
</td>
</tr>


<!--People Watching IJCV-->
<tr><td>
<img src="2014/pwijcv/teaser.jpg">
</td><td>
<b>D. F. Fouhey</b>, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic.<br>
<i>People Watching: Human Actions as a Cue for Single View Geometry</i>.<br>
IJCV (extended version of ECCV 2012 paper)<br>
[<a href="2014/pwijcv/pw_IJCV_preprint.pdf">Preprint (PDF)</a>] &nbsp;
[<a href="http://link.springer.com/article/10.1007/s11263-014-0710-z">Final version (via Springer)</a>]
<br>
</td>
</tr>


<!--Data-driven 3D Primitives-->
<tr><td>
<img src="2013/3dp/teaser.jpg">
</td><td>
<b>D. F. Fouhey</b>, A. Gupta, and M. Hebert<br>
<i>Data-Driven 3D Primitives for Single Image Understanding</i><br>
ICCV 2013<br>
[<a href="2013/3dp/dfouhey_primitives.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2013/3dp/fouhey13.bib">Bibtex</a>] <br>
[<a href="2013/3dp/index.html">Project Page</a>] &nbsp; 
[<a href="2013/3dp/poster_v3.pdf">Poster (PDF)</a>] &nbsp; 
<br>
</td>
</tr>

<!--People Watching-->
<tr><td>

<img src="2012/pw/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic.<br>
<i>People Watching: Human Actions as a Cue for Single View Geometry</i>.<br>
ECCV 2012 (<span style="color: rgb(170, 0, 0); --darkreader-inline-color:#ff5555;" data-darkreader-inline-color=""><b>Oral - 
<a href="http://videolectures.net/eccv2012_fouhey_geometry/">Watch the presentation on VideoLectures.net</a>
</b></span>)<br>
[<a href="2012/pw/dfouhey_people.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2012/pw/fouhey12.bib">Bibtex</a>]<br>
[<a href="http://graphics.cs.cmu.edu/projects/peopleWatching/">Project Page</a>] &nbsp; 
<br>
</td>
</tr>

<!--Scene Semantics-->
<tr><td>
<img src="2012/sem/teaser.jpg" width="300">
</td><td>
V. Delaitre, <b>D. F. Fouhey</b>, I. Laptev, J. Sivic, A. Gupta, and A. Efros.<br>
<i>Scene Semantics from Long-term Observation of People</i>.<br>
ECCV 2012<br>
[<a href="2012/sem/delaitre_ECCV12.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="2012/sem/delaitre12.bib">Bibtex</a>] <br>
[<a href="http://www.di.ens.fr/willow/research/scenesemantics/">Project Page</a>] 
</td>

<!--- MOPED 3D -->
</tr><tr><td>
<img src="2012/objrec/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, A. Collet, M. Hebert, and S. Srinivasa<br>
<i>Object Recognition Robust to Imperfect Depth Data</i>.<br>
CDC4CV 2012 Workshop at ECCV 2012<br>
[<a href="2012/objrec/dfouhey_cdc4cv12.pdf">Publication (PDF)</a>] &nbsp;
[<a href="2012/objrec/fouhey12b.bib">Bibtex</a>]<br>
[<a href="2012/objrec/supp_cdc4cv.pdf">Supplemental(PDF)</a>] &nbsp;
[<a href="2012/objrec/video_qualitative_cdc4cv.avi">Supp. Video 1</a>] &nbsp;
[<a href="2012/objrec/video_run_cdc4cv.avi">Supp. Video 2</a>] &nbsp;
</td></tr>



<!--REV-->
<tr><td>
<img src="earlier/agu/teaser.jpg" width="300">
</td><td>
M. Costanza-Robinson, B. Estabrook, and <b>D. F. Fouhey</b><br>
<i>Representative elementary volume 
estimation for porosity, moisture saturation, and air-water 
interfacial areas in unsaturated porous media: Data quality implications</i><br>
<u>(Sorry for not posting a pre-print!)</u><br>
In Water Resources Research, Volume 47, 2011<br>
[<a href="http://onlinelibrary.wiley.com/doi/10.1029/2010WR009655/abstract">Official Version</a>] &nbsp; 
[<a href="earlier/agu/costanzarobinson11.bib">Bibtex</a>]<br>
</td></tr>



<!--Plane Detection-->
<tr><td>
<img src="earlier/mpd/teaser.jpg" width="300">
</td><td>
<b>D. F. Fouhey</b>, D. Scharstein, and A. Briggs.<br>
<i>Multiple plane detection in image pairs using J-linkage.</i><br>
ICPR 2010<br>
[<a href="earlier/mpd/Fouhey_PlaneDetect_ICPR2010.pdf">Publication (PDF)</a>] &nbsp; 
[<a href="earlier/mpd/fouhey10.bib">Bibtex</a>]<br>
Implementation (Python and C) [<a href="earlier/mpd/MPD.zip">Code (Zip)</a>] &nbsp;
[<a href="earlier/mpd/ICPR2010Poster.pdf">Poster (PDF)</a>] &nbsp; 
</td></tr> 
</tbody></table>
</div>


<a name="miscellaneous"></a>
<div class="mainbox contentbox">
<h3>Miscellaneous</h3>

You may be interested in the following.<br><br>


<b>Writing (arranged in chronological order):</b>
<ul>
<li>My Ph.D. dissertation, 
<i>Factoring Scenes into 3D Structure and Style</i> 
(<a href="2016/dissertation/dissertation.pdf">Dissertation</a> and 
<a href="2016/dissertation/dissertation_talk.pdf">Defense Slides</a>)<br>
</li><li><a href="2016/evalSN/evalSN.html">A note on some practical considerations when evaluating surface normals</a>
</li><li>My A.B. thesis,
<i>Multi-Model Estimation in the Presence of Outliers</i>
(<a href="earlier/thesis/dfouhey_thesis.pdf">Thesis</a> and
<a href="earlier/thesis/dfouhey_thesisPoster.pdf">Poster</a> and
<a href="earlier/thesis/dfouhey_thesisPresentation.pdf">Presentation</a>)
</li></ul>
<b>Miscellaneous</b>
<ul>
<li>
<a href="misc/thesisTemplate/template.zip">CMU RI Thesis Template (zip)</a> based off of
a CMU RI tech report template from Daniel Morris.
</li></ul>
</div>

<a name="fun"></a>
<div class="mainbox contentbox">
<h3>Joke Papers</h3>
<br>
Sometimes when I feel a creative itch, the end result is a joke publication
(which despite the name often have a serious point to make).
These are all done with the one and only 
<a href="http://www.dimatura.net">Daniel Maturana</a>.
<ul>
<li><b>Keras4Kindergartners.com:</b> Check out <a href="http://keras4kindergartners.com/">our secret backup plan</a> for if research doesn't
work out. 
</li><li><b>Deep Excel:</b> Everybody knows that deep learning brings about synergy and so does Excel, 
so 
<a href="http://www.dimatura.net">Daniel Maturana</a> and I released <a href="http://www.deepexcel.net/">ExcelNet</a>, 
a break-through technology that merges the power of Deep Learning with Excel.<br>
<a href="fun/deepexcel/cnn.xls">The spreadsheet</a> &nbsp; 
<a href="fun/deepexcel/paper.pdf">The whitepaper</a> &nbsp; 
<a href="fun/deepexcel/slides.pdf">The pitch slides</a> &nbsp; 
<a href="fun/deepexcel/protips.txt">Protips (actually quite helpful)</a> &nbsp; 
</li><li><b>Visual Rank Estimation:</b> <a href="http://www.oneweirdkerneltrick.com/rank.pdf">Visually Identifying Rank</a>
with <a href="http://www.dimatura.net/">Daniel Maturana</a> proves that linear algebra can be replaced
with machine learning. It also shows that if you are a CNN, the much-hated jet colormap is actually the best colormap.
Winner of the ``Most Frighteningly Like Real Research'' award at SIGBOVIK 2016.
</li><li><b>Cat Basis Purrsuit:</b> I've been told that this is the highlight of my research career
and it can only go downhill from here:
<a href="http://oneweirdkerneltrick.com/catbasis.pdf">Cat Basis Pursuit</a>
</li><li><b>Celebrity Learning:</b> You may also know my award-winning work 
with <a href="http://www.dimatura.net/">Daniel Maturana</a> on celebrity-themed 
learning, making money at home from Hilbert's Nulstellensatz, and more from 
<a href="http://www.oneweirdkerneltrick.com">OneWeirdKernelTrick.com</a> 
</li><li><b>Kardashian Kernel:</b> The original, rarely imitated, never duplicated. Originator
of the alphabetically-related-work section:
<a href="http://oneweirdkerneltrick.com/sigbovik_2012.pdf">The Kardashian Kernel</a>
</li></ul>

</div>


<div class="mainbox contentbox">
<h3>Fun &amp; Games</h3>

<ul>
<li><b>The LaCroix Flavors, Ranked:</b> <a href="fun/lacroix/">Uh, yeah</a>
</li><li><b>Visualizing AUROCs:</b> I could never figure out what an AUROC of 60% actually means. So I made a fun visualization to play with. Play with
it <a href="fun/AUROC/">here</a>.
</li><li><b>Venus Transit Poster:</b> I wanted a poster of the <a href="https://en.wikipedia.org/wiki/Transit_of_Venus">transit of venus</a>, and didn't like
what I could find. So I made one myself with the original <a href="http://jsoc.stanford.edu/">JSOC SDO/AIA data</a>. Here:
<a href="fun/venus_poster_5k.pdf">Highish-res (5k)</a> <a href="fun/venus_poster_8k.pdf">Higher-res (8k)</a>
<a href="fun/venus_poster_7p5k.pdf">Padded to 30"x40"</a> 

</li><li><b>Paris 20/20:</b> The only way to see Paris is to walk through all 20 <a href="https://en.wikipedia.org/wiki/Arrondissements_of_Paris">arrondissements</a>
in a single journey by foot. 
Co-conspirators (so far!): <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a> <a href="http://andrewowens.com/">Andrew Owens</a>.<br>
Possible variants: <a href="https://www.mappedometer.com/?maproute=711518">here</a> 
<a href="https://www.mappedometer.com/?maproute=620052">another</a> 
<a href="https://www.mappedometer.com/?maproute=619319">the originally planned one (and 20 miles too)</a>.
</li><li><b>Warning: Deep Nets!</b> As mentioned in the New Yorker! You too can have
    <a href="fun/deepnets.pdf">a warning sign for your door</a>.
</li><li><b>Free parking in Berkeley:</b> Parking people hate him. Find out how to park on Berkeley's campus
for free with <a href="fun/parking.pdf">this 1 weird trick</a>.
</li><li><b>Caffe64:</b> dependency-free deep learning in &lt; 12KB via the magic of assembly. Clone and
star it <a href="https://github.com/dfouhey/caffe64">here</a>
</li><li><b>miniml:</b> Everybody these days has a deep learning toolkit that takes forever to download, has tons
of dependencies, and produces inscrutable models. In the spirit of simplicity and transparency, I wrote
a new package that's only 2172 bytes when compiled, has zero dependencies (not even the C standard library -- that's total bloat), 
and fits a hidden-layer-free network aka logistic regression. Meet miniml here: <a href="fun/miniml/">miniml</a> .
</li><li><b>Programming in Postscript?:</b> Found in the archives, a postscript file (<a href="fun/can.ps">can.ps</a>)that solves 
<a href="https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem">The Missionaries/Cannibals</a> problem,
then renders the solution. Send it to a printer, open it in vim, be amazed at how much time I had as an undergrad.
</li><li><b>Revisiting Monet in Light of that Other Painter of light:</b> 
Monet is so <i>booooring</i> and tranquil. Click <a href="fun/monet/index.html">here</a> to see him 
spiced up to look like Thomas Kinkade
</li><li><b>Academic Ancestry and Erdös Number:</b> <a href="fun/aancestry/index.htm">See where I fit in!</a>
</li><li><b>Sculpture at Berkeley:</b> <a href="fun/SculpturesOfBerkeley/">3D shapes are fun and Berkeley has a lot of them</a>

</li><li><b>#1 Messiest Desk:</b> My <a href="fun/desk/desk.jpg">desk</a> was <a href="fun/desk/votes.jpg">voted</a> #1 messiest desk in
the second floor of Smith Hall at Carnegie Mellon during the RI's open house for 2016.<br>
If a cluttered desk is an indication of a cluttered mind, then what does an empty desk indicate?


</li><li>
<b>Award-Winning Optimization Sheet:</b> I wrote an award-winning one page 
<a href="fun/review/review.pdf">cheat-sheet</a> for Convex Optimization at CMU (10-725). Be sure to
check the watermark!
</li></ul> 

</div>

</div>


</body>
</html>
